{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Kafka Studies This repository regroup a set of deeper personal studies on kafka. Mirror Maker 2.0 The mirror-maker-2 folder includes the scripts and configurations to support different mirroring strategies. The details are in this note. Kafka Connect with Debezium Kafka with Quarkus Here is a template code for quarkus based Kafka consumer: quarkus-event-driven-consumer-microservice-template . This interesting solution with Quarkus and kafka streaming: Quarkus using Kafka Streams Kafka with Apache Camel 3.0","title":"Introduction"},{"location":"#apache-kafka-studies","text":"This repository regroup a set of deeper personal studies on kafka.","title":"Apache Kafka Studies"},{"location":"#mirror-maker-20","text":"The mirror-maker-2 folder includes the scripts and configurations to support different mirroring strategies. The details are in this note.","title":"Mirror Maker 2.0"},{"location":"#kafka-connect-with-debezium","text":"","title":"Kafka Connect with Debezium"},{"location":"#kafka-with-quarkus","text":"Here is a template code for quarkus based Kafka consumer: quarkus-event-driven-consumer-microservice-template . This interesting solution with Quarkus and kafka streaming: Quarkus using Kafka Streams","title":"Kafka with Quarkus"},{"location":"#kafka-with-apache-camel-30","text":"","title":"Kafka with Apache Camel 3.0"},{"location":"kstreams/","text":"Kafka Streams implementation of the container inventory management In this chapter we are presenting how to sue the Kafka Streams API combined with Kafka event sourcing to implement the container inventory management. The component can be represented in the figure below: For getting started with Kafka Streams API read this tutorial . The container topics includes all the event about container life cycle. The application is Java based and deployed in Liberty packaged into a docker image deployable on Kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in Ktable. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose Start with maven Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId = org.apache.kafka -DarchetypeArtifactId = streams-quickstart-java -DarchetypeVersion = 2 .1.0 -DgroupId = kc-container -DartifactId = kc-container-streams -Dversion = 0 .1 -Dpackage = containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind= \"con\" path= \"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\" > <attributes> <attribute name= \"maven.pomderived\" value= \"true\" /> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> ${kafka.version} </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> ${kafka.version} </version> <scope> test </scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit. Start with IBM Cloud microprofile starter kit Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command: Some useful Kafka streams APIs The stream configuration looks similar to the Kafka consumer and producer configuration. props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"container-streams\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"localhost:9092\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder (); builder . stream ( \"orders\" ) . foreach (( key , value ) -> { Order order = parser . fromJson (( String ) value , OrderEvent . class ). getPayload (); // TODO do something to the order System . out . println ( \"received order \" + key + \" \" + value ); }); final Topology topology = builder . build (); final KafkaStreams streams = new KafkaStreams ( topology , props ); streams . start (); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API, keep this page close to you . Test Driven Development We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment. Container inventory When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig . getStreamsProperties ( \"test\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummy:1234\" ); TopologyTestDriver testDriver = new TopologyTestDriver ( buildProcessFlow (), props ); ConsumerRecordFactory < String , String > factory = new ConsumerRecordFactory < String , String >( \"containers\" , new StringSerializer (), new StringSerializer ()); ConsumerRecord < byte [], byte []> record = factory . create ( \"containers\" , ce . getContainerID (), parser . toJson ( ce )); testDriver . pipeInput ( record ); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore < String , String > store = testDriver . getKeyValueStore ( \"queryable-container-store\" ); String containerStrg = store . get ( ce . getContainerID ()); Assert . assertNotNull ( containerStrg ); Assert . assertTrue ( containerStrg . contains ( ce . getContainerID ())); Assert . assertTrue ( containerStrg . contains ( \"atDock\" )); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow () { final StreamsBuilder builder = new StreamsBuilder (); // containerEvent is a string, map values help to change the type and data of the inpit values builder . stream ( CONTAINERS_TOPIC ). mapValues (( containerEvent ) -> { // the container payload is of interest to keep in table Container c = jsonParser . fromJson (( String ) containerEvent , ContainerEvent . class ). getPayload (); return jsonParser . toJson ( c ); }). groupByKey () // the keys are kept so we can group by key to prepare for the tabl . reduce (( container , container1 ) -> { System . out . println ( \"received container \" + container1 ); return container1 ; }, Materialized . as ( \"queryable-container-store\" )); return builder . build (); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams ; // .. Properties props = ApplicationConfig . getStreamsProperties ( \"container-streams\" ); props . put ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , \"earliest\" ); streams = new KafkaStreams ( buildProcessFlow (), props ); try { streams . cleanUp (); streams . start (); } catch ( Throwable e ) { System . exit ( 1 ); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener { @Override public void contextInitialized ( ServletContextEvent sce ) { // Initialize the Container consumer ContainerInventoryView cView = ( ContainerInventoryView ) ContainerInventoryView . instance (); cView . start (); } Now we can add the getById operation, package as a war, deploy it to Liberty. Container to Order Assignment The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener. Run tests Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest = TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server docker compose configuration Replace existing springcontainerms declaration to the following containerkstreams : image : ibmcase/kc-containerkstreams:latest hostname : containerkstreams ports : - \"12080:9080\" environment : KAFKA_ENV : ${KAFKA_ENV} KAFKA_BROKERS : ${KAFKA_BROKERS} KAFKA_APIKEY : ${KAFKA_APIKEY} How streams flows are resilient? Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list. How to scale?","title":"Kafka streaming notes"},{"location":"kstreams/#kafka-streams-implementation-of-the-container-inventory-management","text":"In this chapter we are presenting how to sue the Kafka Streams API combined with Kafka event sourcing to implement the container inventory management. The component can be represented in the figure below: For getting started with Kafka Streams API read this tutorial . The container topics includes all the event about container life cycle. The application is Java based and deployed in Liberty packaged into a docker image deployable on Kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in Ktable. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose","title":"Kafka Streams implementation of the container inventory management"},{"location":"kstreams/#start-with-maven","text":"Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId = org.apache.kafka -DarchetypeArtifactId = streams-quickstart-java -DarchetypeVersion = 2 .1.0 -DgroupId = kc-container -DartifactId = kc-container-streams -Dversion = 0 .1 -Dpackage = containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind= \"con\" path= \"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\" > <attributes> <attribute name= \"maven.pomderived\" value= \"true\" /> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> ${kafka.version} </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> ${kafka.version} </version> <scope> test </scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit.","title":"Start with maven"},{"location":"kstreams/#start-with-ibm-cloud-microprofile-starter-kit","text":"Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command:","title":"Start with IBM Cloud microprofile starter kit"},{"location":"kstreams/#some-useful-kafka-streams-apis","text":"The stream configuration looks similar to the Kafka consumer and producer configuration. props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"container-streams\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"localhost:9092\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder (); builder . stream ( \"orders\" ) . foreach (( key , value ) -> { Order order = parser . fromJson (( String ) value , OrderEvent . class ). getPayload (); // TODO do something to the order System . out . println ( \"received order \" + key + \" \" + value ); }); final Topology topology = builder . build (); final KafkaStreams streams = new KafkaStreams ( topology , props ); streams . start (); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API, keep this page close to you .","title":"Some useful Kafka streams APIs"},{"location":"kstreams/#test-driven-development","text":"We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment.","title":"Test Driven Development"},{"location":"kstreams/#container-inventory","text":"When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig . getStreamsProperties ( \"test\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummy:1234\" ); TopologyTestDriver testDriver = new TopologyTestDriver ( buildProcessFlow (), props ); ConsumerRecordFactory < String , String > factory = new ConsumerRecordFactory < String , String >( \"containers\" , new StringSerializer (), new StringSerializer ()); ConsumerRecord < byte [], byte []> record = factory . create ( \"containers\" , ce . getContainerID (), parser . toJson ( ce )); testDriver . pipeInput ( record ); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore < String , String > store = testDriver . getKeyValueStore ( \"queryable-container-store\" ); String containerStrg = store . get ( ce . getContainerID ()); Assert . assertNotNull ( containerStrg ); Assert . assertTrue ( containerStrg . contains ( ce . getContainerID ())); Assert . assertTrue ( containerStrg . contains ( \"atDock\" )); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow () { final StreamsBuilder builder = new StreamsBuilder (); // containerEvent is a string, map values help to change the type and data of the inpit values builder . stream ( CONTAINERS_TOPIC ). mapValues (( containerEvent ) -> { // the container payload is of interest to keep in table Container c = jsonParser . fromJson (( String ) containerEvent , ContainerEvent . class ). getPayload (); return jsonParser . toJson ( c ); }). groupByKey () // the keys are kept so we can group by key to prepare for the tabl . reduce (( container , container1 ) -> { System . out . println ( \"received container \" + container1 ); return container1 ; }, Materialized . as ( \"queryable-container-store\" )); return builder . build (); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams ; // .. Properties props = ApplicationConfig . getStreamsProperties ( \"container-streams\" ); props . put ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , \"earliest\" ); streams = new KafkaStreams ( buildProcessFlow (), props ); try { streams . cleanUp (); streams . start (); } catch ( Throwable e ) { System . exit ( 1 ); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener { @Override public void contextInitialized ( ServletContextEvent sce ) { // Initialize the Container consumer ContainerInventoryView cView = ( ContainerInventoryView ) ContainerInventoryView . instance (); cView . start (); } Now we can add the getById operation, package as a war, deploy it to Liberty.","title":"Container inventory"},{"location":"kstreams/#container-to-order-assignment","text":"The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener.","title":"Container to Order Assignment"},{"location":"kstreams/#run-tests","text":"Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest = TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server","title":"Run tests"},{"location":"kstreams/#docker-compose-configuration","text":"Replace existing springcontainerms declaration to the following containerkstreams : image : ibmcase/kc-containerkstreams:latest hostname : containerkstreams ports : - \"12080:9080\" environment : KAFKA_ENV : ${KAFKA_ENV} KAFKA_BROKERS : ${KAFKA_BROKERS} KAFKA_APIKEY : ${KAFKA_APIKEY}","title":"docker compose configuration"},{"location":"kstreams/#how-streams-flows-are-resilient","text":"Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list.","title":"How streams flows are resilient?"},{"location":"kstreams/#how-to-scale","text":"","title":"How to scale?"},{"location":"mirrormaker/","text":"Mirror Maker 2.0 Studies Mirror Maker 2.0 is the new replication feature of Kafka 2.4. In this note we are presenting different test scenario for topic replication. Replicate from local cluster to Event Streams on Cloud (See detail in the scenario 1 section ) Replicate from Strimzi 'local' Kafka cluster running on OpenShift to Event Streams on Cloud. (See detail in the scenario 2 section ) Replicate from Event Streams on cloud being the source cluster to local Kafka cluster running on local machine (started via docker-compose) using Strimzi Kafka docker image. Replicate from Event Streams on premise running on Openshift being the source cluster to Event Stream on the Cloud as target cluster . Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks The mirror-maker-2 folder includes, scripts, code and configurations to support those scenarios. Pre-requisites You need to have one Event Streams service created on IBM Cloud. You may need to use Event Streams CLI. So follow those instructions to get it. The following ibmcloud CLI command presents the Event Stream cluster's metadata, like the broker list and the cluster ID: ibmcloud es cluster For other CLI commands see this summary . To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. When the Event Streams service is created, add a service credentials and get the brokers list and api key information. We will use them in a setenv.sh script file under mirror-maker-2 folder to define environment variables. General concepts As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and define source and target cluster with their connection properties and the replication flow definitions. Here is a simple example clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and use Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] but can be also specified with the properties: topics.blacklist . Comma-separated lists are also supported. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with assign API and so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Scenario 1: From Kafka local as source to Event Streams on Cloud as Target The scenario is to send the products definition in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh to source broker is your local cluster, and the target is event streams with its API KEY export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" Create the target topics in the target cluster. The following topics needs to be created upfront in events streams as Access Control does not authorize program to create topic dynamically. ibmcloud es topic-create -n mm2-configs.source.internal -p 1 -c cleanup.policy=compact ibmcloud es topic-create -n mm2-offsets.source.internal -p 25 -c cleanup.policy=compact ibmcloud es topic-create -n mm2-status.source.internal -p 5 -c cleanup.policy=compact ibmcloud es topic-create -n source.products -p 1 ibmcloud es topic-create -n source.heartbeats -p 1 ibmcloud es topic-create -n source.checkpoints.internal -p 1 -c cleanup.policy=compact In one window terminal, start local cluster using docker-compose under the mirror-maker-2/local-cluster folder: docker-compose up & . The data are persisted on the local disk in this folder. If this is the first time you started the cluster, you need to create the products topic. Start a Kafka container to access the Kafka tool with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify it is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command build this image (if you change the image name be sure to use the new name in future command): docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In the container bash shell do the following $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams properties file for the tool command. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to event streams and with the replicated topic: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0 in a local docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This script is updating a template properties file with the values of the environment variables and call /opt/kafka/bin/connect-mirror-maker.sh The trace includes a ton of messages, which demonstrate different consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving after replication. Scenario 2: Run Mirror Maker 2 Cluster close to target cluster So the scenario is similar in term of test as the scenario 1 but Mirror Maker runs within an OpenShift cluster in the same data center as Event Streams cluster. We have created an Event Streams cluster on Washington DC data center. We have a Strimzi Kafka cluster defined in Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target cluster (Event Streams on Cloud). Producers are running locally on the same openshift cluster as a pod, or can run remotely using exposed Openshift route. What needs to be done: Get a Openshift cluster in the same data center as Event Streams service: See this product introduction . Create a project in OpenShift, for example: mirror-maker-2-to-es . Deploy the Strimzi Kafka cluster and topic operators. See the sections on role binding, cluster operator, topic operator and user operator deployments from the deployment note . The 0.17.0 source is in this repository , unzip and use the install folder + strimzi instructions. The service account and role binding do not need to be installed if you did it previously. If not done yet, create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-apikey-target --from-literal=binding=<replace-with-event-streams-apikey> Define source and target cluster properties in mirror maker 2.0 kafka-to-es-mm2.yml file: apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaMirrorMaker2 metadata : name : mm2-cluster namespace : jb-kafka-strimzi spec : version : 2.4.0 replicas : 1 connectCluster : \"mm2-cluster\" clusters : - alias : \"kafka-on-premise-cluster-source\" bootstrapServers : my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 tls : trustedCertificates : - secretName : my-cluster-cluster-ca-cert certificate : ca.crt - alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 authentication : username : token passwordSecret : secretName : es-apikey-target password : password type : plain config : config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 mirrors : - sourceCluster : \"kafka-on-premise-cluster-source\" targetCluster : \"event-streams-wdc-as-target\" heartbeatConnector : config : heartbeats.topic.replication.factor : 1 checkpointConnector : config : checkpoints.topic.replication.factor : 1 topicsPattern : \"products\" groupsPattern : \".*\" Deploy Mirror maker 2.0 within this project oc apply -f kafka-to-es-mm2.yaml Define a target cluster property file: bootstrap.servers = broker-3-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.cert\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS jbcodeforce/python37 bash python SendProductToKafka.py ./data/products2.json As an alternate to this external producer, we can start a producer as pod inside Openshift, and then send the product one by one: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don 't see a command prompt, try pressing enter. >{' product_id ': ' P01 ', ' description ': ' Carrots ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P02 ', ' description ': ' Banana ', ' target_temperature ': 6, ' target_humidity_level ': 0.6, ' content_type ': 2} >{' product_id ': ' P03 ', ' description ': ' Salad ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P04 ', ' description ': ' Avocado ', ' target_temperature ': 6, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P05 ', ' description ': ' Tomato ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ' : 2 } Scenario 3: From Event Streams to local cluster For the first test the source is Event Streams on IBM Cloud and the target is a local server (may be on a laptop using Kafka strimzi images started with docker compose) This time the producer adds headers and message and Mirror maker need to get the APIkey, so the mirror-maker.properties looks like: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders We have created an Event Streams cluster on Washington DC data center. We have a Strimzi Kafka cluster defined in Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target cluster (Event Streams on Cloud). Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift Scenario 5: From event streams on premise to event streams on cloud The source cluster is a Strimzi cluster running on Openshift as a service on IBM Cloud. It was installed following the instructions documented here . The target cluster is also based on Strimzi kafka 2.4 docker image, but run in a local host, with docker compose. It starts two zookeeper nodes, and three kafka nodes. We need 3 kafka brokers as mirror maker created topics with a replication factor set to 3. Start the target cluster runnning on your laptop using: docker-compose up Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate: clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 source.security.protocol = SSL source.ssl.truststore.password = password source.ssl.truststore.location = /home/truststore.jks target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders As the source cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by those Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. When running from a remote system to get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt For Openshift or Kubernetes deployment, the mirror maker descriptor needs to declare the TLS stamza: mirrors : - sourceCluster : \"my-cluster-source\" targetCluster : \"my-cluster-target\" sourceConnector : config : replication.factor : 1 offset-syncs.topic.replication.factor : 1 sync.topic.acls.enabled : \"false\" targetConnector : tls : trustedCertificates : - secretName : my-cluster-cluster-cert certificate : ca.crt The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders Typical errors in Mirror Maker 2 traces Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is the name topic is not created. In Event Streams topics needs to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114)","title":"Mirror Maker 2.0"},{"location":"mirrormaker/#mirror-maker-20-studies","text":"Mirror Maker 2.0 is the new replication feature of Kafka 2.4. In this note we are presenting different test scenario for topic replication. Replicate from local cluster to Event Streams on Cloud (See detail in the scenario 1 section ) Replicate from Strimzi 'local' Kafka cluster running on OpenShift to Event Streams on Cloud. (See detail in the scenario 2 section ) Replicate from Event Streams on cloud being the source cluster to local Kafka cluster running on local machine (started via docker-compose) using Strimzi Kafka docker image. Replicate from Event Streams on premise running on Openshift being the source cluster to Event Stream on the Cloud as target cluster . Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks The mirror-maker-2 folder includes, scripts, code and configurations to support those scenarios.","title":"Mirror Maker 2.0 Studies"},{"location":"mirrormaker/#pre-requisites","text":"You need to have one Event Streams service created on IBM Cloud. You may need to use Event Streams CLI. So follow those instructions to get it. The following ibmcloud CLI command presents the Event Stream cluster's metadata, like the broker list and the cluster ID: ibmcloud es cluster For other CLI commands see this summary . To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. When the Event Streams service is created, add a service credentials and get the brokers list and api key information. We will use them in a setenv.sh script file under mirror-maker-2 folder to define environment variables.","title":"Pre-requisites"},{"location":"mirrormaker/#general-concepts","text":"As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and define source and target cluster with their connection properties and the replication flow definitions. Here is a simple example clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and use Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] but can be also specified with the properties: topics.blacklist . Comma-separated lists are also supported. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with assign API and so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern.","title":"General concepts"},{"location":"mirrormaker/#scenario-1-from-kafka-local-as-source-to-event-streams-on-cloud-as-target","text":"The scenario is to send the products definition in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh to source broker is your local cluster, and the target is event streams with its API KEY export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" Create the target topics in the target cluster. The following topics needs to be created upfront in events streams as Access Control does not authorize program to create topic dynamically. ibmcloud es topic-create -n mm2-configs.source.internal -p 1 -c cleanup.policy=compact ibmcloud es topic-create -n mm2-offsets.source.internal -p 25 -c cleanup.policy=compact ibmcloud es topic-create -n mm2-status.source.internal -p 5 -c cleanup.policy=compact ibmcloud es topic-create -n source.products -p 1 ibmcloud es topic-create -n source.heartbeats -p 1 ibmcloud es topic-create -n source.checkpoints.internal -p 1 -c cleanup.policy=compact In one window terminal, start local cluster using docker-compose under the mirror-maker-2/local-cluster folder: docker-compose up & . The data are persisted on the local disk in this folder. If this is the first time you started the cluster, you need to create the products topic. Start a Kafka container to access the Kafka tool with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify it is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command build this image (if you change the image name be sure to use the new name in future command): docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In the container bash shell do the following $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams properties file for the tool command. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to event streams and with the replicated topic: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0 in a local docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This script is updating a template properties file with the values of the environment variables and call /opt/kafka/bin/connect-mirror-maker.sh The trace includes a ton of messages, which demonstrate different consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving after replication.","title":"Scenario 1: From Kafka local as source to Event Streams on Cloud as Target"},{"location":"mirrormaker/#scenario-2-run-mirror-maker-2-cluster-close-to-target-cluster","text":"So the scenario is similar in term of test as the scenario 1 but Mirror Maker runs within an OpenShift cluster in the same data center as Event Streams cluster. We have created an Event Streams cluster on Washington DC data center. We have a Strimzi Kafka cluster defined in Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target cluster (Event Streams on Cloud). Producers are running locally on the same openshift cluster as a pod, or can run remotely using exposed Openshift route. What needs to be done: Get a Openshift cluster in the same data center as Event Streams service: See this product introduction . Create a project in OpenShift, for example: mirror-maker-2-to-es . Deploy the Strimzi Kafka cluster and topic operators. See the sections on role binding, cluster operator, topic operator and user operator deployments from the deployment note . The 0.17.0 source is in this repository , unzip and use the install folder + strimzi instructions. The service account and role binding do not need to be installed if you did it previously. If not done yet, create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-apikey-target --from-literal=binding=<replace-with-event-streams-apikey> Define source and target cluster properties in mirror maker 2.0 kafka-to-es-mm2.yml file: apiVersion : kafka.strimzi.io/v1alpha1 kind : KafkaMirrorMaker2 metadata : name : mm2-cluster namespace : jb-kafka-strimzi spec : version : 2.4.0 replicas : 1 connectCluster : \"mm2-cluster\" clusters : - alias : \"kafka-on-premise-cluster-source\" bootstrapServers : my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 tls : trustedCertificates : - secretName : my-cluster-cluster-ca-cert certificate : ca.crt - alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 authentication : username : token passwordSecret : secretName : es-apikey-target password : password type : plain config : config.storage.replication.factor : 1 offset.storage.replication.factor : 1 status.storage.replication.factor : 1 mirrors : - sourceCluster : \"kafka-on-premise-cluster-source\" targetCluster : \"event-streams-wdc-as-target\" heartbeatConnector : config : heartbeats.topic.replication.factor : 1 checkpointConnector : config : checkpoints.topic.replication.factor : 1 topicsPattern : \"products\" groupsPattern : \".*\" Deploy Mirror maker 2.0 within this project oc apply -f kafka-to-es-mm2.yaml Define a target cluster property file: bootstrap.servers = broker-3-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-q.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.cert\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS jbcodeforce/python37 bash python SendProductToKafka.py ./data/products2.json As an alternate to this external producer, we can start a producer as pod inside Openshift, and then send the product one by one: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don 't see a command prompt, try pressing enter. >{' product_id ': ' P01 ', ' description ': ' Carrots ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P02 ', ' description ': ' Banana ', ' target_temperature ': 6, ' target_humidity_level ': 0.6, ' content_type ': 2} >{' product_id ': ' P03 ', ' description ': ' Salad ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P04 ', ' description ': ' Avocado ', ' target_temperature ': 6, ' target_humidity_level ': 0.4, ' content_type ': 1} >{' product_id ': ' P05 ', ' description ': ' Tomato ', ' target_temperature ': 4, ' target_humidity_level ': 0.4, ' content_type ' : 2 }","title":"Scenario 2: Run Mirror Maker 2 Cluster close to target cluster"},{"location":"mirrormaker/#scenario-3-from-event-streams-to-local-cluster","text":"For the first test the source is Event Streams on IBM Cloud and the target is a local server (may be on a laptop using Kafka strimzi images started with docker compose) This time the producer adds headers and message and Mirror maker need to get the APIkey, so the mirror-maker.properties looks like: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders We have created an Event Streams cluster on Washington DC data center. We have a Strimzi Kafka cluster defined in Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target cluster (Event Streams on Cloud).","title":"Scenario 3: From Event Streams to local cluster"},{"location":"mirrormaker/#scenario-4-from-event-streams-on-cloud-to-strimzi-cluster-on-openshift","text":"","title":"Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift"},{"location":"mirrormaker/#scenario-5-from-event-streams-on-premise-to-event-streams-on-cloud","text":"The source cluster is a Strimzi cluster running on Openshift as a service on IBM Cloud. It was installed following the instructions documented here . The target cluster is also based on Strimzi kafka 2.4 docker image, but run in a local host, with docker compose. It starts two zookeeper nodes, and three kafka nodes. We need 3 kafka brokers as mirror maker created topics with a replication factor set to 3. Start the target cluster runnning on your laptop using: docker-compose up Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate: clusters = source, target source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 source.security.protocol = SSL source.ssl.truststore.password = password source.ssl.truststore.location = /home/truststore.jks target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders As the source cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by those Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. When running from a remote system to get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt For Openshift or Kubernetes deployment, the mirror maker descriptor needs to declare the TLS stamza: mirrors : - sourceCluster : \"my-cluster-source\" targetCluster : \"my-cluster-target\" sourceConnector : config : replication.factor : 1 offset-syncs.topic.replication.factor : 1 sync.topic.acls.enabled : \"false\" targetConnector : tls : trustedCertificates : - secretName : my-cluster-cluster-cert certificate : ca.crt The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders","title":"Scenario 5: From event streams on premise to event streams on cloud"},{"location":"mirrormaker/#typical-errors-in-mirror-maker-2-traces","text":"Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is the name topic is not created. In Event Streams topics needs to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114)","title":"Typical errors in Mirror Maker 2 traces"},{"location":"python/","text":"Kafka programming with python There are a lot of python based code already defined in this EDA reference project and integration tests. Basic consumer and producer In the python-kafka folder, I used a very simple setting to run kafka locally with docker compose and python environment. KafkaConsumer.py is for used to connect to Kafka brokers, which URL is defined in environment variable (KAFKA_BROKERS) and uses the confluent_kafka library. To run this consumer using local kafka, first start kafka and zookeeper using docker compose: cd python-kafka docker-compose up & docker rm Env1 ./startPythonDocker.sh Env1 root@19d06f7d163e:/home# cd python-kafka/ root@19d06f7d163e:/home/python-kafka# python KafkaConsumer.py The producer code KafkaProducer.py is in separate program and run in a second container docker rm Env2 ./startPythonDocker.sh Env2 5001 root@44e827a5c2cc:/home# cd python-kafka/ root@44e827a5c2cc:/home/python-kafka# python KafkaProducer.py Using event streams on Cloud Set the KAFKA_BROKERS to the brokers URL of the event streams instance. The setenv.sh is used for that: root@44e827a5c2cc:/home/kafka# source ./setenv.sh IBMCLOUD echo $KAFKA_BROKERS python KafkaConsumer.py # or python KafkaProducer.py Faust The other API to integrate with Kafka is the Faust for streamings. To execute the first basic faust agent and producer code use the following: FaustEasiestApp.py As previously start a docker python container and then: root@44e827a5c2cc:/home# pip install faust & cd python-kafka root@44e827a5c2cc:/home/python-kafka# faust -A FaustEasiestApp worker -l info","title":"Kafka with Python"},{"location":"python/#kafka-programming-with-python","text":"There are a lot of python based code already defined in this EDA reference project and integration tests.","title":"Kafka programming with python"},{"location":"python/#basic-consumer-and-producer","text":"In the python-kafka folder, I used a very simple setting to run kafka locally with docker compose and python environment. KafkaConsumer.py is for used to connect to Kafka brokers, which URL is defined in environment variable (KAFKA_BROKERS) and uses the confluent_kafka library. To run this consumer using local kafka, first start kafka and zookeeper using docker compose: cd python-kafka docker-compose up & docker rm Env1 ./startPythonDocker.sh Env1 root@19d06f7d163e:/home# cd python-kafka/ root@19d06f7d163e:/home/python-kafka# python KafkaConsumer.py The producer code KafkaProducer.py is in separate program and run in a second container docker rm Env2 ./startPythonDocker.sh Env2 5001 root@44e827a5c2cc:/home# cd python-kafka/ root@44e827a5c2cc:/home/python-kafka# python KafkaProducer.py","title":"Basic consumer and producer"},{"location":"python/#using-event-streams-on-cloud","text":"Set the KAFKA_BROKERS to the brokers URL of the event streams instance. The setenv.sh is used for that: root@44e827a5c2cc:/home/kafka# source ./setenv.sh IBMCLOUD echo $KAFKA_BROKERS python KafkaConsumer.py # or python KafkaProducer.py","title":"Using event streams on Cloud"},{"location":"python/#faust","text":"The other API to integrate with Kafka is the Faust for streamings. To execute the first basic faust agent and producer code use the following: FaustEasiestApp.py As previously start a docker python container and then: root@44e827a5c2cc:/home# pip install faust & cd python-kafka root@44e827a5c2cc:/home/python-kafka# faust -A FaustEasiestApp worker -l info","title":"Faust"},{"location":"strimzi-deploy/","text":"Strimzi Kafka deployment on Openshift or Kubernetes Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration. Create a namespace or openshift project kubectl create namespace jb-kafka-strimzi oc create project jb-kafka-strimzi Download the strimzi artefacts For the last release github . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml Define Custom Resource Definition for kafka oc apply -f install/cluster-operator/ -n jb-kafka-strimzi This should create the following resources: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition Deploy Kafka cluster Change the name of the cluster in one the yaml in the examples/kafka folder. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod. Using persistence: oc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi Topic Operator The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics. Deploy the operator oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition Create a topic Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster. Test with producer and consumer pods Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka yaml file include the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Strimzi deployment"},{"location":"strimzi-deploy/#strimzi-kafka-deployment-on-openshift-or-kubernetes","text":"Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka or Kafka Connect cluster configuration.","title":"Strimzi Kafka deployment on Openshift or Kubernetes"},{"location":"strimzi-deploy/#create-a-namespace-or-openshift-project","text":"kubectl create namespace jb-kafka-strimzi oc create project jb-kafka-strimzi","title":"Create a namespace or openshift project"},{"location":"strimzi-deploy/#download-the-strimzi-artefacts","text":"For the last release github . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: kafka-strimzi/' install/cluster-operator/*RoleBinding*.yaml","title":"Download the strimzi artefacts"},{"location":"strimzi-deploy/#define-custom-resource-definition-for-kafka","text":"oc apply -f install/cluster-operator/ -n jb-kafka-strimzi This should create the following resources: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definition oc get customresourcedefinition","title":"Define Custom Resource Definition for kafka"},{"location":"strimzi-deploy/#deploy-kafka-cluster","text":"Change the name of the cluster in one the yaml in the examples/kafka folder. Using non presistence: oc apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 # Or kubectl apply -f examples/kafka/kafka-ephemeral.yaml -n jb-kafka-strimzi When looking at the pods running we can see the three kafka and zookeeper nodes, but also an entity operator pod. Using persistence: oc apply -f examples/kafka/kafka-persistent.yaml -n jb-kafka-strimzi","title":"Deploy Kafka cluster"},{"location":"strimzi-deploy/#topic-operator","text":"The role of the Topic Operator is to keep a set of KafkaTopic OpenShift or Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics.","title":"Topic Operator"},{"location":"strimzi-deploy/#deploy-the-operator","text":"oc apply -f install/topic-operator/ -n jb-kafka-strimzi This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition","title":"Deploy the operator"},{"location":"strimzi-deploy/#create-a-topic","text":"Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : my-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml -n jb-kafka-strimzi oc get kafkatopics This creates a topic test in your kafka cluster.","title":"Create a topic"},{"location":"strimzi-deploy/#test-with-producer-and-consumer-pods","text":"Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts locally but remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka yaml file include the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol=SSL --producer-property ssl.truststore.password=password --producer-property ssl.truststore.location=/home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Test with producer and consumer pods"}]}